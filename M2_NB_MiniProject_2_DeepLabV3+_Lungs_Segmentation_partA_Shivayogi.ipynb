{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "iQIXL-qvIHRD"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aisha-partha/AIMLOps-MiniProjects/blob/mp_4/M2_NB_MiniProject_2_DeepLabV3%2B_Lungs_Segmentation_partA_Shivayogi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Certification Programme in AI and MLOps\n",
        "## A programme by IISc and TalentSprint\n",
        "### Mini-Project: Lung Segmentation of Chest X-Ray dataset using DeepLabV3+"
      ],
      "metadata": {
        "id": "LJY-HzQzCiJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "- understand, prepare, and visualize the the dataset containing image and corresponding masked image used for segmentation\n",
        "- implement DeepLabV3+ architecture\n",
        "- create a masked image (prediction)"
      ],
      "metadata": {
        "id": "DHVXkSrlCq6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "Semantic segmentation is a computer vision task that involves dividing an image into different regions, each of which is labeled with a semantic category. The goal of semantic segmentation is to enable machines to understand the content of an image at a pixel level, by assigning a label to each individual pixel based on the object or region it belongs to.\n",
        "\n",
        "This technique is widely used in many applications such as self-driving cars, medical image analysis, and object recognition in robotics. It helps to extract meaningful information from images and to understand the relationships between objects and their environment.\n",
        "\n",
        "The below figure shows how semantic segmentation differs from other algorithms, such as object detection.\n",
        "\n",
        "<br>\n",
        "<img src='https://cdn.iisc.talentsprint.com/AIandMLOps/Images/segmentation_vs_other_algos.jpeg' width=700px>\n",
        "<br><br>\n",
        "\n",
        "Moreover, in contrast to object detection, which detects and localizes objects within an image, semantic segmentation is more precise and detailed. It provides a much more granular understanding of the content of an image, allowing for more advanced and accurate applications."
      ],
      "metadata": {
        "id": "gOGH5iqfz8ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "*  The Chest X-Ray dataset is made up of images and segmentated mask from two diffrent sources - Shenzhen and Montgomery dataset\n",
        "*  The CXR_png folder consists of Chest X-Rays and the masks folder has the segmented mask\n",
        "*  There are 704 images with their masks mapped with each other  \n",
        "\n",
        "<br>\n",
        "$\\quad$<img src='https://cdn.extras.talentsprint.com/AIandMLOps/Images/semantic_segmentation.png' width=700px>\n",
        "<br><br>\n"
      ],
      "metadata": {
        "id": "665oBbYDnVgl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "\n",
        "Perfrom the lungs segmentation on Chest X-Ray dataset using DeepLabV3+ model."
      ],
      "metadata": {
        "id": "Y90Rgk5wG5Gc"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "operating-latter"
      },
      "source": [
        "## Grading = 10 Points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "812a816f",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70246b6e-31ae-49e4-e02b-16b7343df2f1"
      },
      "source": [
        "#@title Download the data\n",
        "!wget https://cdn.iisc.talentsprint.com/AIandMLOps/MiniProjects/Datasets/LungSegmentation.zip\n",
        "!unzip -qq LungSegmentation.zip\n",
        "print(\"Data Downloaded Successfuly!!\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-30 09:38:11--  https://cdn.iisc.talentsprint.com/AIandMLOps/MiniProjects/Datasets/LungSegmentation.zip\n",
            "Resolving cdn.iisc.talentsprint.com (cdn.iisc.talentsprint.com)... 172.105.52.210\n",
            "Connecting to cdn.iisc.talentsprint.com (cdn.iisc.talentsprint.com)|172.105.52.210|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 139904834 (133M) [application/zip]\n",
            "Saving to: ‘LungSegmentation.zip’\n",
            "\n",
            "LungSegmentation.zi 100%[===================>] 133.42M  10.8MB/s    in 16s     \n",
            "\n",
            "2024-05-30 09:38:28 (8.42 MB/s) - ‘LungSegmentation.zip’ saved [139904834/139904834]\n",
            "\n",
            "Data Downloaded Successfuly!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import required packages"
      ],
      "metadata": {
        "id": "JC_nWtDfcjRt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "j1s-j0pdIRKV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "from scipy.io import loadmat\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PART-A**"
      ],
      "metadata": {
        "id": "nL9DKRevMyu0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Visualization (1 point)"
      ],
      "metadata": {
        "id": "iQIXL-qvIHRD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Image Visualization\n",
        "Visualize one of the Chest X-Ray image and its segmented label image."
      ],
      "metadata": {
        "id": "QpD0NpBLkyMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize an image\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "ISbeYklKCLfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize a semantic part segmentation label image\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "ClhuMTOPDXBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJaGbpopIRKW"
      },
      "source": [
        "### Create a TensorFlow Dataset (1 points)\n",
        "Use the image size, batch size, number of classes, and data directory as given in the below code cell.\n",
        "\n",
        "Hint:\n",
        "1. Find all the image files in the CXR_png and masks subdirectory separately. The resulting list should be sorted in ascending order.\n",
        "2. Create a list of validation image, validation masks, test image, test masks, etc.\n",
        "3. Create a function that reads an image file and returns a preprocessed image tensor.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "8sj6gK1vIRKW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f543b3c2-9345-4413-a1fd-698906225e8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CHNCXR_0370_1.png', 'CHNCXR_0371_1.png', 'CHNCXR_0372_1.png', 'CHNCXR_0373_1.png', 'CHNCXR_0374_1.png', 'CHNCXR_0375_1.png', 'CHNCXR_0376_1.png', 'CHNCXR_0377_1.png', 'CHNCXR_0378_1.png', 'CHNCXR_0379_1.png', 'CHNCXR_0380_1.png', 'CHNCXR_0381_1.png', 'CHNCXR_0382_1.png', 'CHNCXR_0383_1.png', 'CHNCXR_0384_1.png', 'CHNCXR_0385_1.png', 'CHNCXR_0386_1.png', 'CHNCXR_0387_1.png', 'CHNCXR_0388_1.png', 'CHNCXR_0389_1.png', 'CHNCXR_0390_1.png', 'CHNCXR_0391_1.png', 'CHNCXR_0392_1.png', 'CHNCXR_0393_1.png', 'CHNCXR_0394_1.png', 'CHNCXR_0395_1.png', 'CHNCXR_0396_1.png', 'CHNCXR_0397_1.png', 'CHNCXR_0398_1.png', 'CHNCXR_0399_1.png', 'CHNCXR_0400_1.png', 'CHNCXR_0401_1.png', 'CHNCXR_0402_1.png', 'CHNCXR_0403_1.png', 'CHNCXR_0404_1.png', 'CHNCXR_0405_1.png', 'CHNCXR_0406_1.png', 'CHNCXR_0407_1.png', 'CHNCXR_0408_1.png', 'CHNCXR_0409_1.png', 'CHNCXR_0410_1.png', 'CHNCXR_0411_1.png', 'CHNCXR_0412_1.png', 'CHNCXR_0413_1.png', 'CHNCXR_0414_1.png', 'CHNCXR_0415_1.png', 'CHNCXR_0416_1.png', 'CHNCXR_0417_1.png', 'CHNCXR_0418_1.png', 'CHNCXR_0419_1.png', 'CHNCXR_0420_1.png', 'CHNCXR_0421_1.png', 'CHNCXR_0422_1.png', 'CHNCXR_0423_1.png', 'CHNCXR_0424_1.png', 'CHNCXR_0425_1.png', 'CHNCXR_0426_1.png', 'CHNCXR_0427_1.png', 'CHNCXR_0428_1.png', 'CHNCXR_0429_1.png', 'CHNCXR_0430_1.png', 'CHNCXR_0431_1.png', 'CHNCXR_0432_1.png', 'CHNCXR_0433_1.png', 'CHNCXR_0434_1.png', 'CHNCXR_0435_1.png', 'CHNCXR_0436_1.png', 'CHNCXR_0437_1.png', 'CHNCXR_0438_1.png', 'CHNCXR_0439_1.png', 'CHNCXR_0440_1.png', 'CHNCXR_0441_1.png', 'CHNCXR_0442_1.png', 'CHNCXR_0443_1.png', 'CHNCXR_0444_1.png', 'CHNCXR_0445_1.png', 'CHNCXR_0446_1.png', 'CHNCXR_0447_1.png', 'CHNCXR_0448_1.png', 'CHNCXR_0449_1.png', 'CHNCXR_0450_1.png', 'CHNCXR_0451_1.png', 'CHNCXR_0452_1.png', 'CHNCXR_0453_1.png', 'CHNCXR_0454_1.png', 'CHNCXR_0455_1.png', 'CHNCXR_0456_1.png', 'CHNCXR_0457_1.png', 'CHNCXR_0458_1.png', 'CHNCXR_0459_1.png', 'CHNCXR_0460_1.png', 'CHNCXR_0461_1.png', 'CHNCXR_0462_1.png', 'CHNCXR_0463_1.png', 'CHNCXR_0464_1.png', 'CHNCXR_0465_1.png', 'CHNCXR_0466_1.png', 'CHNCXR_0467_1.png', 'CHNCXR_0468_1.png']\n",
            "['CHNCXR_0370_1.png', 'CHNCXR_0371_1.png', 'CHNCXR_0372_1.png', 'CHNCXR_0373_1.png', 'CHNCXR_0374_1.png', 'CHNCXR_0375_1.png', 'CHNCXR_0376_1.png', 'CHNCXR_0377_1.png', 'CHNCXR_0378_1.png', 'CHNCXR_0379_1.png', 'CHNCXR_0380_1.png', 'CHNCXR_0381_1.png', 'CHNCXR_0382_1.png', 'CHNCXR_0383_1.png', 'CHNCXR_0384_1.png', 'CHNCXR_0385_1.png', 'CHNCXR_0386_1.png', 'CHNCXR_0387_1.png', 'CHNCXR_0388_1.png', 'CHNCXR_0389_1.png', 'CHNCXR_0390_1.png', 'CHNCXR_0391_1.png', 'CHNCXR_0392_1.png', 'CHNCXR_0393_1.png', 'CHNCXR_0394_1.png', 'CHNCXR_0395_1.png', 'CHNCXR_0396_1.png', 'CHNCXR_0397_1.png', 'CHNCXR_0398_1.png', 'CHNCXR_0399_1.png', 'CHNCXR_0400_1.png', 'CHNCXR_0401_1.png', 'CHNCXR_0402_1.png', 'CHNCXR_0403_1.png', 'CHNCXR_0404_1.png', 'CHNCXR_0405_1.png', 'CHNCXR_0406_1.png', 'CHNCXR_0407_1.png', 'CHNCXR_0408_1.png', 'CHNCXR_0409_1.png', 'CHNCXR_0410_1.png', 'CHNCXR_0411_1.png', 'CHNCXR_0412_1.png', 'CHNCXR_0413_1.png', 'CHNCXR_0414_1.png', 'CHNCXR_0415_1.png', 'CHNCXR_0416_1.png', 'CHNCXR_0417_1.png', 'CHNCXR_0418_1.png', 'CHNCXR_0419_1.png', 'CHNCXR_0420_1.png', 'CHNCXR_0421_1.png', 'CHNCXR_0422_1.png', 'CHNCXR_0423_1.png', 'CHNCXR_0424_1.png', 'CHNCXR_0425_1.png', 'CHNCXR_0426_1.png', 'CHNCXR_0427_1.png', 'CHNCXR_0428_1.png', 'CHNCXR_0429_1.png', 'CHNCXR_0430_1.png', 'CHNCXR_0431_1.png', 'CHNCXR_0432_1.png', 'CHNCXR_0433_1.png', 'CHNCXR_0434_1.png', 'CHNCXR_0435_1.png', 'CHNCXR_0436_1.png', 'CHNCXR_0437_1.png', 'CHNCXR_0438_1.png', 'CHNCXR_0439_1.png', 'CHNCXR_0440_1.png', 'CHNCXR_0441_1.png', 'CHNCXR_0442_1.png', 'CHNCXR_0443_1.png', 'CHNCXR_0444_1.png', 'CHNCXR_0445_1.png', 'CHNCXR_0446_1.png', 'CHNCXR_0447_1.png', 'CHNCXR_0448_1.png', 'CHNCXR_0449_1.png', 'CHNCXR_0450_1.png', 'CHNCXR_0451_1.png', 'CHNCXR_0452_1.png', 'CHNCXR_0453_1.png', 'CHNCXR_0454_1.png', 'CHNCXR_0455_1.png', 'CHNCXR_0456_1.png', 'CHNCXR_0457_1.png', 'CHNCXR_0458_1.png', 'CHNCXR_0459_1.png', 'CHNCXR_0460_1.png', 'CHNCXR_0461_1.png', 'CHNCXR_0462_1.png', 'CHNCXR_0463_1.png', 'CHNCXR_0464_1.png', 'CHNCXR_0465_1.png', 'CHNCXR_0466_1.png', 'CHNCXR_0467_1.png', 'CHNCXR_0468_1.png']\n"
          ]
        }
      ],
      "source": [
        "IMAGE_SIZE = 512\n",
        "BATCH_SIZE = 4\n",
        "NUM_CLASSES = 2\n",
        "DATA_DIR = \"./LungSegmentation\"\n",
        "\n",
        "# Find paths for all the image files in the 'CXR_png' subdirectory of the DATA_DIR directory.\n",
        "# Sort the list of file paths in ascending order\n",
        "#all_images = # YOUR CODE HERE\n",
        "import os\n",
        "import re\n",
        "\n",
        "def find_numbered_files(directory):\n",
        "    pattern = re.compile(r'\\d+')\n",
        "    numbered_files = [f for f in os.listdir(directory) if pattern.search(f)]\n",
        "    return numbered_files\n",
        "\n",
        "# Replace '/path/to/folder' with the actual path to your directory\n",
        "#directory_path = '/path/to/folder'\n",
        "all_images = sorted(find_numbered_files(DATA_DIR + \"/CXR_png\"))\n",
        "\n",
        "#print(\"Numbered files in the folder:\")\n",
        "#for file in all_images:\n",
        "#    print(file)\n",
        "\n",
        "\n",
        "# Find paths for all the mask files in the 'masks' subdirectory of the DATA_DIR directory.\n",
        "# Sort the list of file paths in ascending order\n",
        "all_masks = sorted(find_numbered_files(DATA_DIR + \"/masks\"))\n",
        "#print(\"Numbered files in the folder:\")\n",
        "#for file in all_masks:\n",
        "#    print(file)\n",
        "\n",
        "\n",
        "# Create a list of validation image files\n",
        "val_images = []\n",
        "for file in all_images[301:400]:\n",
        "    val_images.append(file)\n",
        "print(val_images)\n",
        "\n",
        "\n",
        "\n",
        "# Create a list of validation mask files\n",
        "val_masks = []\n",
        "for file in all_masks[301:400]:\n",
        "    val_masks.append(file)\n",
        "print(val_masks)\n",
        "\n",
        "# Create a list of test image files\n",
        "test_images = []\n",
        "for file in all_images[201:300]:\n",
        "    test_images.append(file)\n",
        "\n",
        "# Create a list of test mask files\n",
        "test_masks = []\n",
        "for file in all_masks[201:300]:\n",
        "    test_masks.append(file)\n",
        "\n",
        "\n",
        "# Create a list of train image files\n",
        "train_images = []\n",
        "for file in all_images[:200]:\n",
        "    train_images.append(file)\n",
        "\n",
        "# Create a list of train mask files\n",
        "train_masks = []\n",
        "for file in all_masks[:200]:\n",
        "    train_masks.append(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to read an image file and returns a preprocessed image tensor.\n",
        "# The mask argument is set to False by default, indicating that it is an image file, not a mask file.\n",
        "\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "'''\n",
        "def read_image(image_path, mask=False):\n",
        "    \"\"\"\n",
        "    Load an image file and preprocess it into a tensor.\n",
        "\n",
        "    Args:\n",
        "    image_path (str): Path to the image file.\n",
        "    target_size (tuple): The target size of the image after resizing (height, width).\n",
        "\n",
        "    Returns:\n",
        "    tf.Tensor: Preprocessed image tensor.\n",
        "    \"\"\"\n",
        "    # Load the image file\n",
        "    image = Image.open(image_path)\n",
        "    # Resize the image\n",
        "    image = image.resize((224,224)), Image.ANTIALIAS)\n",
        "    # Convert the image to a numpy array and normalize the pixel values\n",
        "    image_array = np.array(image) / 255.0\n",
        "    # Convert the numpy array to a TensorFlow tensor\n",
        "    image_tensor = tf.convert_to_tensor(image_array, dtype=tf.float32)\n",
        "    # Add a batch dimension\n",
        "    image_tensor = tf.expand_dims(image_tensor, axis=0)\n",
        "    return image_tensor\n",
        "\n",
        "# Example usage\n",
        "image_tensor = read_image(DATA_DIR + \"/CXR_png\" + \"/CHNCXR_0010_0.png\" )\n",
        "print(image_tensor.shape)\n",
        "\n",
        "'''\n",
        "\n",
        "def read_image(file_path):\n",
        "    # Reads an image from a file, decodes it into a dense tensor, and resizes it\n",
        "    # to a fixed shape.\n",
        "    image = tf.io.read_file(file_path)\n",
        "    image = tf.image.decode_png(image, channels=3)  # Adjust channels according to your image format\n",
        "    image = tf.image.resize(image, [256, 256])  # Resize images if required\n",
        "    return image\n",
        "\n"
      ],
      "metadata": {
        "id": "VprZ9E7gDN2P"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a **load_data** function that takes in two arguments, **image_list** and **mask_list**, which are lists of file paths to the images and corresponding masks, respectively. It then reads in the image and mask using the **read_image** function defined earlier. The function returns a tuple of **image** and **mask**."
      ],
      "metadata": {
        "id": "b7MxNDd33VDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(image_path, mask_path):\n",
        "    # Load the image and mask and prepare them for processing.\n",
        "    image = read_image(image_path)\n",
        "    mask = read_image(mask_path)\n",
        "    return image, mask"
      ],
      "metadata": {
        "id": "1c0ahnXm3SKL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a **data_generator** function that takes in **image_list** and **mask_list** as arguments.\n",
        "\n",
        "The function should:\n",
        "- create a **tf.data.Dataset** object from the input data\n",
        "- map the **load_data** function to each element in the dataset\n",
        "- convert the dataset into batches of size **BATCH_SIZE**, drop any incomplete batch at the end of the dataset\n",
        "- return the resulting dataset"
      ],
      "metadata": {
        "id": "RKh_mhVP3VdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate train and validation set\n",
        "\n",
        "def data_generator(image_list, mask_list, BATCH_SIZE):\n",
        "    # Create a tf.data.Dataset from image and mask paths.\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((image_list, mask_list))\n",
        "    # Apply the load_data function to each item in the dataset.\n",
        "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    # Batch the data and drop any incomplete batches.\n",
        "    dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "    # Prefetch data for enhanced performance.\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset\n",
        "\n",
        "# Example usage\n",
        "train_image_list = [DATA_DIR  + \"/CXR_png\" + \"/CHNCXR_0010_0.png\", DATA_DIR + \"/CXR_png\" + \"/CHNCXR_0011_0.png\"]  # Add your actual image paths here\n",
        "train_mask_list = [DATA_DIR + \"/masks\" + \"/CHNCXR_0010_0.png\" , DATA_DIR + \"/masks\" + \"/CHNCXR_0011_0.png\"]  # Add your actual mask paths here\n",
        "BATCH_SIZE = 2  # Set your batch size as needed\n",
        "\n",
        "dataset = data_generator(train_image_list, train_mask_list, BATCH_SIZE)\n",
        "for images, masks in dataset.take(1):  # Process one batch\n",
        "    print(\"train_dataset_img = \" +  str(images.shape))\n",
        "    print(\"train_dataset_msk = \" + str(masks.shape))  # Print shapes to verify\n",
        "batch_count_train = sum(1 for _ in dataset)\n",
        "print(batch_count_train)\n",
        "\n",
        "#batch_count = dataset.cardinality().numpy()\n",
        "#print(\"Total number of batches:\", batch_count_train)\n",
        "\n",
        "\n",
        "val_img_list = [DATA_DIR  + \"/CXR_png/\" + val_images[5], DATA_DIR  + \"/CXR_png/\" + val_images[6]]\n",
        "val_msk_list = [DATA_DIR  + \"/masks/\" + val_masks[5], DATA_DIR  + \"/masks/\" + val_masks[6]]\n",
        "\n",
        "dataset = data_generator(val_img_list, val_msk_list, BATCH_SIZE)\n",
        "for images, masks in dataset.take(1):  # Process one batch\n",
        "    print(\"val_dataset_img = \" + str(images.shape))\n",
        "    print(\"val_dataset_msk = \" + str(masks.shape))  # Print shapes to verify\n",
        "\n",
        "batch_count_val = sum(1 for _ in dataset)\n",
        "print(batch_count_val)\n",
        "\n",
        "#train_dataset = data_generator(# YOUR CODE HERE)\n",
        "#val_dataset = data_generator(# YOUR CODE HERE)"
      ],
      "metadata": {
        "id": "7-4Z2Vgx3ULA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cddc2aa4-6941-4f21-af0f-21b619692632"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_dataset_img = (2, 256, 256, 3)\n",
            "train_dataset_msk = (2, 256, 256, 3)\n",
            "1\n",
            "val_dataset_img = (2, 256, 256, 3)\n",
            "val_dataset_msk = (2, 256, 256, 3)\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the total images for train and validation."
      ],
      "metadata": {
        "id": "dEqbkpyw2TUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "imgs_train = 2\n",
        "imgs_val = 2\n",
        "total_images = imgs_train + imgs_val\n"
      ],
      "metadata": {
        "id": "vBr67cusqgfu"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check the number of batches formed in train and validation set.\n"
      ],
      "metadata": {
        "id": "n4g4SWTf2doR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE\n",
        "total_batches = batch_count_train + batch_count_val\n",
        "print(total_batches)"
      ],
      "metadata": {
        "id": "yVWuKj1SpsO9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a773aed6-5c51-4494-9bed-5ae9b599c604"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have created our TensorFlow Dataset. Further, let us try to understand the DeepLabV3+ model."
      ],
      "metadata": {
        "id": "od4kRMIt4bBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **PART-B**"
      ],
      "metadata": {
        "id": "3QD5CLTxM6XN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the DeepLabV3+ model"
      ],
      "metadata": {
        "id": "rb9boz_8rA2e"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEPE701wIRKX"
      },
      "source": [
        "Downsampling is widely adopted in deep convolutional neural networks (CNN) for reducing memory consumption while preserving the transformation invariance to some degree.\n",
        "\n",
        "Multiple downsampling of a CNN will lead the feature map resolution to become smaller, resulting in lower prediction accuracy and loss of boundary information in semantic segmentation.\n",
        "\n",
        "DeepLabv3+ helps in solving these issues by including **atrous convolutions**. They aggregate context around a feature which helps in segmenting it better.\n",
        "\n",
        "<br>\n",
        "\n",
        "#### **Atrous Convolution/Dilated Convolution**\n",
        "\n",
        "It is a tool for refining the effective field of view of the convolution. It modifies the field of view using a parameter termed ***atrous rate*** or ***dilation rate***.\n",
        "\n",
        "With dilated convolution, as we go deeper in the network, we can keep the stride constant but with larger field-of-view without increasing the number of parameters or the amount of computation. It also enables larger output feature maps, which is useful for semantic segmentation.\n",
        "\n",
        "In the below figure, Atrous/Dilated Convolution has wider field of view with same number of parameters as Normal convolution.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/Dilated_Conv.jpg\" width=500px>\n",
        "<br><br>\n",
        "\n",
        "\n",
        "\n",
        "#### **DeepLabv3+**\n",
        "\n",
        "Earlier version, DeepLabv3 has a problem of consuming too much time to process high-resolution images. DeepLabv3+ is a semantic segmentation architecture that improves upon DeepLabv3 with several improvements, such as adding an effective decoder module to refine the segmentation results.\n",
        "\n",
        "The below figure shows the typical architecture of DeepLabv3+. The encoder module processes multiscale contextual information by applying dilated/atrous convolution at multiple scales, while the decoder module refines the segmentation results along object boundaries.\n",
        "\n",
        "<br>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/AIandMLOps/Images/deeplabv3_plus_model.png\" width=1000px>\n",
        "<br><br>\n",
        "\n",
        "Deeplabv3+ employs Aligned Xception network as its main feature extractor (encoder), although with substantial modifications. Depth-wise separable convolution replaces all max pooling procedures.\n",
        "\n",
        "In Model Playground, we can select feature extraction (encoding) network to use as either **Resnet** or EfficientNet.\n",
        "\n",
        "The reason for using **Dilated Spatial Pyramid Pooling** is that it was shown that as the sampling rate becomes larger, the number of valid filter weights (i.e., weights that are applied to the valid feature region, instead of padded zeros) becomes smaller.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the model (5 points)\n",
        "First, create different functions  to implement DeepLabV3+ architecture.\n",
        "\n"
      ],
      "metadata": {
        "id": "8-D80Tn5Vb8I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a function, **convolution_block()**, to add a convolution layer, a BatchNormalization layer, and apply ReLu activation in one go."
      ],
      "metadata": {
        "id": "51JFQYafBWKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "swj5dBq3Nuhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create another function to perform **Dilated Spatial Pyramid Pooling**. Use above function to add different convolution blocks."
      ],
      "metadata": {
        "id": "oZ97G7zKDEwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ak0-MaLIRKX"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISzIfZnCIRKY"
      },
      "source": [
        "The encoder features are first bilinearly upsampled by a factor 4, and then concatenated with the corresponding low-level features from the network backbone that have the same spatial resolution. Here, we use a **ResNet50** pretrained on ImageNet as the backbone model, and we use\n",
        "the low-level features from the `conv4_block6_2_relu` block of the backbone.\n",
        "\n",
        "##### Exploring ResNet-50 architecture before using it."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res_input = keras.Input(shape=(128, 128, 3))\n",
        "resnet50 = keras.applications.ResNet50(weights=\"imagenet\", include_top=False, input_tensor = res_input)\n",
        "\n",
        "# Layers present in ResNet-50 network\n",
        "resnet50.summary()"
      ],
      "metadata": {
        "id": "drTp8R3jJ8l5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above layers,\n",
        "\n",
        "- Use the low-level features from the `conv2_block3_2_relu` layer of the ResNet-50 network to fead in Decoder.\n",
        "\n",
        "- Use the features from the `conv4_block6_2_relu` layer of the ResNet-50 to fead in Dilated Spatial Pyramid Pooling module."
      ],
      "metadata": {
        "id": "pkc9lfo1KCmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Encoder\n",
        "Create a function to implement the architecture for Encoder block. Use **ResNet50** pretrained on ImageNet as the backbone network. Use the features from the **conv4_block6_2_relu** layer of the backbone to fead in Dilated Spatial Pyramid Pooling module. Then return the backbone network along with encoder output."
      ],
      "metadata": {
        "id": "O3FUKLkfKleN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "F55hQulMN85C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create Decoder\n",
        "\n",
        "Create a function to implement the architecture for Decoder block. The encoder features are first bilinearly upsampled by a factor 4, and then concatenated with the corresponding low-level features (the **conv2_block3_2_relu** layer) from the network backbone that have the same spatial resolution."
      ],
      "metadata": {
        "id": "3Tr-1r0iKpSe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WPwmoG9IRKY"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building full architecture of the Model\n",
        "\n",
        "Write a function that combines the encoder and decoder functions defined above to create & implement a complete  DeepLabV3+ architecture."
      ],
      "metadata": {
        "id": "5_Q4iEL_LAS_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KKrCbtbTLt4V"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create model"
      ],
      "metadata": {
        "id": "tgxmWewiL_3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-wTaL2GIRKZ"
      },
      "source": [
        "### Training (1 point)\n",
        "\n",
        "We train the model using sparse categorical crossentropy as the loss function, and\n",
        "Adam as the optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Compile model\n"
      ],
      "metadata": {
        "id": "lzc1PwXGVxGA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "lHOIcWvJJipS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Model Training"
      ],
      "metadata": {
        "id": "Lfxei5y7Vpbi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X5RJA-4IRKZ"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot the training and validation loss to see how the training went. This should show generally decreasing values per epoch."
      ],
      "metadata": {
        "id": "EFI7dNzjls1n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####  Learning curve from model history (1 point)"
      ],
      "metadata": {
        "id": "mZXQrtWilsys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot learning curves\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "CK39p3vukEUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display learning curves\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "wKrXpv_fs5Rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Predictions (1 points)"
      ],
      "metadata": {
        "id": "xyE-Kbx2MT2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference from model\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "PKjiu5Bih-pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Plot the predictions"
      ],
      "metadata": {
        "id": "v9_RcKasWLGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "VOUW1R60sh76"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}